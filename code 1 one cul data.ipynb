{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      " [ -24.  -22.  -17. ... -155.    6. -221.]\n",
      ";;;;;;;;;;;;;;;;;\n",
      " [1. 1. 1. ... 3. 3. 3.]\n",
      "++++++++++++++++++++++++++++++++\n",
      "\n",
      "(1229100,)\n",
      "(1229100,)\n",
      "New x_train shape:  (983280, 1, 1) New y_train shape:  (983280, 1)\n",
      "New x_train shape:  (983280, 1, 1) New y_train shape:  (983280, 3)\n",
      "3\n",
      "New x_test shape:  (245820, 1, 1) New y_test shape:  (245820, 1, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jojo/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:87: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(4, 6, strides=1, activation=\"relu\", input_shape=(1, 1), data_format=\"channels_first\", padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 4, 1)              28        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1, 1)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 4)              24        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 1, 4)              0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1, 4)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 10)             170       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 1, 10)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1, 10)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1, 10)             410       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1, 10)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1, 10)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 1, 15)             615       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1, 15)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 1, 15)             0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                800       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 3,130\n",
      "Trainable params: 3,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 786624 samples, validate on 196656 samples\n",
      "Epoch 1/150\n",
      "786624/786624 [==============================] - 17s 21us/step - loss: 0.8898 - acc: 0.5402 - val_loss: 0.8863 - val_acc: 0.5445\n",
      "Epoch 2/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8851 - acc: 0.5438 - val_loss: 0.8850 - val_acc: 0.5447\n",
      "Epoch 3/150\n",
      "786624/786624 [==============================] - 9s 12us/step - loss: 0.8849 - acc: 0.5437 - val_loss: 0.8842 - val_acc: 0.5455\n",
      "Epoch 4/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8848 - acc: 0.5437 - val_loss: 0.8839 - val_acc: 0.5459\n",
      "Epoch 5/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8847 - acc: 0.5440 - val_loss: 0.8849 - val_acc: 0.5449\n",
      "Epoch 6/150\n",
      "786624/786624 [==============================] - 9s 12us/step - loss: 0.8846 - acc: 0.5439 - val_loss: 0.8839 - val_acc: 0.5458\n",
      "Epoch 7/150\n",
      "786624/786624 [==============================] - 9s 12us/step - loss: 0.8846 - acc: 0.5439 - val_loss: 0.8843 - val_acc: 0.5448\n",
      "Epoch 8/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8845 - acc: 0.5441 - val_loss: 0.8845 - val_acc: 0.5446\n",
      "Epoch 9/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8846 - acc: 0.5439 - val_loss: 0.8839 - val_acc: 0.5455\n",
      "Epoch 10/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8845 - acc: 0.5440 - val_loss: 0.8839 - val_acc: 0.5452\n",
      "Epoch 11/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8844 - acc: 0.5441 - val_loss: 0.8842 - val_acc: 0.5453\n",
      "Epoch 12/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8844 - acc: 0.5442 - val_loss: 0.8847 - val_acc: 0.5438\n",
      "Epoch 13/150\n",
      "786624/786624 [==============================] - 9s 12us/step - loss: 0.8844 - acc: 0.5439 - val_loss: 0.8837 - val_acc: 0.5457\n",
      "Epoch 14/150\n",
      "786624/786624 [==============================] - 9s 12us/step - loss: 0.8844 - acc: 0.5440 - val_loss: 0.8842 - val_acc: 0.5455\n",
      "Epoch 15/150\n",
      "786624/786624 [==============================] - 9s 12us/step - loss: 0.8844 - acc: 0.5442 - val_loss: 0.8842 - val_acc: 0.5456\n",
      "Epoch 16/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8843 - acc: 0.5440 - val_loss: 0.8845 - val_acc: 0.5455\n",
      "Epoch 17/150\n",
      "786624/786624 [==============================] - 9s 12us/step - loss: 0.8844 - acc: 0.5439 - val_loss: 0.8839 - val_acc: 0.5455\n",
      "Epoch 18/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8843 - acc: 0.5443 - val_loss: 0.8839 - val_acc: 0.5451\n",
      "Epoch 19/150\n",
      "786624/786624 [==============================] - 9s 12us/step - loss: 0.8842 - acc: 0.5443 - val_loss: 0.8838 - val_acc: 0.5456\n",
      "Epoch 20/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8843 - acc: 0.5443 - val_loss: 0.8841 - val_acc: 0.5454\n",
      "Epoch 21/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8843 - acc: 0.5440 - val_loss: 0.8840 - val_acc: 0.5453\n",
      "Epoch 22/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8842 - acc: 0.5441 - val_loss: 0.8838 - val_acc: 0.5455\n",
      "Epoch 23/150\n",
      "786624/786624 [==============================] - 9s 12us/step - loss: 0.8842 - acc: 0.5443 - val_loss: 0.8840 - val_acc: 0.5456\n",
      "Epoch 24/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8843 - acc: 0.5443 - val_loss: 0.8839 - val_acc: 0.5456\n",
      "Epoch 25/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8843 - acc: 0.5441 - val_loss: 0.8837 - val_acc: 0.5457\n",
      "Epoch 26/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8843 - acc: 0.5440 - val_loss: 0.8846 - val_acc: 0.5447\n",
      "Epoch 27/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8843 - acc: 0.5441 - val_loss: 0.8839 - val_acc: 0.5457\n",
      "Epoch 28/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8842 - acc: 0.5442 - val_loss: 0.8838 - val_acc: 0.5457\n",
      "Epoch 29/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8842 - acc: 0.5444 - val_loss: 0.8837 - val_acc: 0.5456\n",
      "Epoch 30/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8842 - acc: 0.5440 - val_loss: 0.8842 - val_acc: 0.5450\n",
      "Epoch 31/150\n",
      "786624/786624 [==============================] - 9s 11us/step - loss: 0.8842 - acc: 0.5442 - val_loss: 0.8840 - val_acc: 0.5451\n",
      "Epoch 32/150\n",
      "786624/786624 [==============================] - 9s 12us/step - loss: 0.8843 - acc: 0.5441 - val_loss: 0.8847 - val_acc: 0.5456\n",
      "Epoch 33/150\n",
      "786624/786624 [==============================] - 9s 12us/step - loss: 0.8842 - acc: 0.5443 - val_loss: 0.8839 - val_acc: 0.5455\n",
      "Epoch 34/150\n",
      "786624/786624 [==============================] - 9s 12us/step - loss: 0.8842 - acc: 0.5441 - val_loss: 0.8841 - val_acc: 0.5456\n",
      "Epoch 35/150\n",
      "786624/786624 [==============================] - 9s 12us/step - loss: 0.8842 - acc: 0.5443 - val_loss: 0.8839 - val_acc: 0.5456\n",
      "Epoch 36/150\n",
      "570400/786624 [====================>.........] - ETA: 2s - loss: 0.8844 - acc: 0.5437"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape\n",
    "from keras.layers import Conv1D, MaxPooling1D,GlobalAveragePooling1D\n",
    "from keras.layers import Input, Flatten, Dense, Dropout,Concatenate\n",
    "#from keras.layers import LeakyReLU\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#dataset = pd.read_csv(\"sof.csv\")\n",
    "#x=dataset.iloc[0:409698,:].values\n",
    "#y=dataset.iloc[409700:,:].values\n",
    "\n",
    "#x = dataset[0:409699,:]\n",
    "#y = dataset[409700:,:]\n",
    "#y=dataset[0,1:3]\n",
    "#y = y.transpose()\n",
    "\n",
    "\"\"\"\n",
    "dataset = np.loadtxt(\"sof.csv\",delimiter=\",\")\n",
    "x = dataset[0:409699,:]\n",
    "y = dataset[409700:,0:3]\n",
    "\n",
    "x=dataset[0:409700,[0,2,4]]\n",
    "y=dataset[0:409700,[1,3,5]]\n",
    "\n",
    "print(y)\n",
    "\"\"\"\n",
    "\n",
    "#lables=[]\n",
    "\n",
    "dataset = np.loadtxt(\"sof copy.csv\",delimiter=\",\")\n",
    "\n",
    "x=dataset[0:409700,0]\n",
    "s=dataset[0:409700,2]\n",
    "o=dataset[0:409700,4]\n",
    "fet=np.concatenate((x, s,o), axis=None)\n",
    "\n",
    "y=dataset[0:409700,1]\n",
    "e=dataset[0:409700,3]\n",
    "u=dataset[0:409700,5]\n",
    "\n",
    "lables=np.concatenate((y, e,u), axis=None)\n",
    "\n",
    "print(\"========\\n\",fet)\n",
    "print(\";;;;;;;;;;;;;;;;;\\n\",lables)\n",
    "\n",
    "\n",
    "print(\"++++++++++++++++++++++++++++++++\\n\")\n",
    "print(lables.shape)\n",
    "\n",
    "\n",
    "print(fet.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(fet,lables, test_size=0.2, random_state=0)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],1,1)\n",
    "y_train = y_train.reshape(y_train.shape[0],1)\n",
    "print('New x_train shape: ', x_train.shape , 'New y_train shape: ',y_train.shape )\n",
    "\n",
    "y_train_hot = np_utils.to_categorical(y_train-1,3)\n",
    "print('New x_train shape: ', x_train.shape , 'New y_train shape: ',y_train_hot.shape )\n",
    "\n",
    "print (x_train.ndim)\n",
    "\n",
    "\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0],1)\n",
    "x_test = x_test.reshape(x_test.shape[0],x_test.shape[1],1)\n",
    "\n",
    "y_test = y_test.reshape(y_test.shape[0],1)\n",
    "y_test = y_test.reshape(y_test.shape[0],y_test.shape[1],1)\n",
    "\n",
    "y_test_hot = np_utils.to_categorical(y_test-1,3)\n",
    "print('New x_test shape: ', x_test.shape , 'New y_test shape: ',y_test_hot.shape )\n",
    "\n",
    "\n",
    "\n",
    "model= Sequential()\n",
    "model.add(Conv1D(4, 6, strides=1,activation='relu',border_mode='same',input_shape=(1,1), data_format='channels_first'))\n",
    "#model.add(Conv1D(4, 6, strides=1, activation='relu', border_mode='same'))\n",
    "model.add(MaxPooling1D(4, strides=2))\n",
    "#model.add(Dropout(0.15))\n",
    "model.add(Conv1D(4, 5, strides=1, activation='relu',padding='same',))\n",
    "#model.add(Conv1D(4, 5, strides=1, activation='relu', padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling1D(4, strides=2, padding='same'))\n",
    "#model.add(Dropout(0.15))\n",
    "model.add(Conv1D(10, 4,strides=1 , activation='relu', padding='same'))\n",
    "#model.add(Conv1D(10, 4,strides=1 , activation='relu', padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling1D(10, strides=2, padding='same'))\n",
    "#model.add(Dropout(0.15))\n",
    "model.add(Conv1D(10, 4, strides=1 , activation='relu', padding='same'))\n",
    "#model.add(Conv1D(10, 4, strides=1 , activation='relu', padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling1D(10, strides=2, padding='same'))\n",
    "#model.add(Dropout(0.15))\n",
    "model.add(Conv1D(15, 4, strides=1 , activation='relu', padding='same'))\n",
    "#model.add(Conv1D(15, 4, strides=1 , padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling1D(15, strides=2 , padding='same'))\n",
    "#model.add(Dropout(0.15))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "#model.add(Dropout(0.15))\n",
    "model.add(Dense (50 , activation='relu'))\n",
    "#model.add(Dropout(0.15))\n",
    "model.add(Dense (20 , activation='relu'))\n",
    "#model.add(Dropout(0.15))\n",
    "model.add(Dense (3,activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "\"\"\"\n",
    "model= Sequential()\n",
    "model.add(Conv1D(4, 6, strides=1,input_shape=(1,3)))\n",
    "model.add(MaxPooling1D(4, strides=2))\n",
    "model.add(Conv1D(4, 5, strides=1, padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling1D(4, strides=2))\n",
    "model.add(Conv1D(10, 4,strides=1 , padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling1D(10, strides=2))\n",
    "\n",
    "model.add(Conv1D(10, 4, strides=1 , padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling1D(10, strides=2))\n",
    "model.add(Conv1D(15, 4, strides=1 , padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling1D(15, strides=2 , padding='same'))\n",
    "\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense (50))\n",
    "model.add(Dense (20))\n",
    "model.add(Dense (5,activation='softmax'))\n",
    "print(model.summary())\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history=model.fit(x_train, y_train_hot, batch_size = 400, epochs=150, validation_split=0.2, verbose=1)\n",
    "\n",
    "# evaluate the model\n",
    "#scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "\n",
    "#print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#cvscores.append(scores[1] * 100)\n",
    "#print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))\n",
    "\n",
    "#print(\"\\n--- Fit the model ---\\n\")\n",
    "\n",
    "# evaluate the model\n",
    "scores=model.evaluate(x_test, y_test_hot, verbose=1)\n",
    "\n",
    "\n",
    "#print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#cvscores.append(scores[1] * 100)\n",
    "#print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))\n",
    "\n",
    "#print(\"\\n--- Fit the model ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
